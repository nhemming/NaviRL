"""
Describes the base class for all learning agents. Each agent has a learning algorithm, an entity(ies) that
it applies too, and an action operation that converts raw neural network outputs to entity state changes.
"""

# native modules
from abc import ABC, abstractmethod
from collections import OrderedDict
import os

# 3rd party modules
import numpy as np
import pandas as pd
import torch.optim as optim

# own modules


class BaseLearningAlgorithm(ABC):

    def __init__(self, device, exploration_strategy, general_params, name, observation_information):

        self.device = device
        self.exploration_strategy = exploration_strategy
        self.name = name
        self.observation_information = observation_information

        # save the general learning parameters
        self.gamma = general_params['gamma']
        self.batch_size = general_params['batch_size']
        self.num_batches = general_params['num_batches']

        # optimizer
        self.optimizer = OrderedDict()

        # save the tuples generated by the agent
        self.history_samples = []

        # save meta data about training the agent
        self.history_train = []

        self.state_info = OrderedDict()
        self.action_info = OrderedDict()

        self.last_reset_time = -np.infty

    @abstractmethod
    def train(self, ep_num, file_path):
        pass

    @abstractmethod
    def save_model(self,ep_num, file_path):
        pass

    @abstractmethod
    def update_memory(self, action_operation, done, entities, reward, sensors, sim_time):

        # if is_new_action == True save data, else don't save the data

        # set is_new_action to false.
        pass

    @abstractmethod
    def load_networks(self, model_path, model_num):
        pass

    def forward(self,state):
        """
        runs the neural network and returns the output
        :param state:
        :return: raw action vector
        """
        pass

    def reset(self):
        # can be overridden by child classes
        self.reset_base()

    def reset_base(self):
        self.history_samples = []
        self.last_reset_time = -np.infty

    def normalize_state(self,entities, sensors):
        """
        Convert the numbers form the dimensional values in the simulation that are needed by the learning agent
        to a different scale. Typically the scales is [0,1] or [-1,1]

        :param entities:
        :param sensors:
        :return:
        """

        norm_state_dict = OrderedDict()
        state_dict = OrderedDict()
        for head_name, head_obs in self.observation_information.items():

            state = np.zeros(len(head_obs))
            norm_state = np.zeros(len(head_obs))
            for index, row in head_obs.iterrows():
                item = entities.get(row['name'],None)
                if item is None:
                    item = sensors.get(row['name'],None)
                norm_state[index] = ((item.state_dict[row['data']]-row['min']) / (row['max'] - row['min'])) * (row['norm_max']-row['norm_min']) + row['norm_min']
                state[index] = item.state_dict[row['data']]
            norm_state_dict[head_name] = norm_state
            state_dict[head_name] = state

        return norm_state_dict, state_dict

    def add_sample_history(self, reward, done, sim_time):
        # init tuple
        mdp_tuple = OrderedDict()
        # add action information
        self.add_sample_history_helper(mdp_tuple, self.action_info)
        # add state information
        self.add_sample_history_helper(mdp_tuple, self.state_info)
        # add reward
        mdp_tuple['reward'] = reward
        # add terminal
        mdp_tuple['done'] = done
        # add sim time
        mdp_tuple['sim_time'] = sim_time

        self.history_samples.append(mdp_tuple)

    def add_sample_history_helper(self, mdp_tuple, info):
        for name, value in info.items():
            if hasattr(value, '__iter__'):

                if isinstance(value, dict):
                    for i, item in value.items():

                        if hasattr(item, '__iter__'):
                            for k, tmp_item in enumerate(item):
                                mdp_tuple[name +'_'+ str(i)+'_'+str(k) ] = tmp_item
                        else:
                            mdp_tuple[name+'_'+str(i)] = item
                else:
                    # iterate through array and add to dict
                    for i, item in enumerate(value):
                        mdp_tuple[name + '_' + str(i)] = item
            else:
                mdp_tuple[name] = value

    def write_history(self,agent_name,episode_number, file_path, eval_num=''):
        # TODO change from csv to sqlite data base
        # write history to csv
        df = pd.DataFrame(self.history_samples)
        file_path = os.path.join(file_path, 'learning_algorithm')
        df.to_csv(os.path.abspath(os.path.join(file_path,str(agent_name)+'_'+str(self.name)+'_epnum-'+str(episode_number)+eval_num+'.csv')), index=False)

    def load_optimizer(self, name, opt_dict, params):
        #opt = self.h_params['Optimizer']
        if opt_dict['type'] == 'adam':
            self.optimizer[name] = optim.Adam(params,lr=opt_dict['lr'],betas=(opt_dict['beta1'],opt_dict['beta2']))
        else:
            raise ValueError('Optimizer not supported')

    def execute_action_operation(self, action_operation, delta_t, controlled_entity, entities, sensors):

        # scale the output to the robots action?
        self.action_info['applied_action'] = action_operation.convert_action(self.action_info['mutated_action'],delta_t, entities, sensors)

        # apply the action
        entities[controlled_entity].apply_action(self.action_info['applied_action'])

    def create_loss_file(self, base_dir, agent):
        file_path = os.path.join(base_dir,'learning_algorithm',agent+'_'+self.name+'_loss.csv')
        self.file_path_loss = file_path
        with open(file_path, 'w') as f:
            f.write(self.loss_header)

    def append_to_loss_file(self, ep_num, loss):
        #loss_string = ''

        tmp_df = pd.DataFrame()
        tmp_df['Episode_number'] = np.ones_like(loss[list(loss.keys())[0]])*ep_num
        for name, value in loss.items():
            tmp_df['loss_'+name] = value

        #for i, loss_val in enumerate(loss): # TODO update to have loss be a dictionary
        #    loss_string += str(ep_num) +','+str(loss_val)+'\n'

        tmp_df.to_csv(self.file_path_loss, mode='a', index=False, header=False)

        #with open(self.file_path_loss, 'a') as f:
        #    f.write(loss_string)
