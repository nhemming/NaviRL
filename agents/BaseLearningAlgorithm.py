"""
Describes the base class for all learning agents. Each agent has a learning algorithm, an entity(ies) that
it applies too, and an action operation that converts raw neural network outputs to entity state changes.
"""

# native modules
from abc import ABC, abstractmethod
import os

# 3rd party modules
import numpy as np
import pandas as pd
import torch.optim as optim

# own modules


class BaseLearningAlgorithm(ABC):

    def __init__(self, device, exploration_strategy, general_params, name, observation_information):

        self.device = device
        self.exploration_strategy = exploration_strategy
        self.name = name
        self.observation_information = observation_information

        # save the general learning parameters
        self.gamma = general_params['gamma']
        self.batch_size = general_params['batch_size']
        self.num_batches = general_params['num_batches']

        # optimizer
        self.optimizer = dict()

        # save the tuples generated by the agent
        self.history_samples = []

        # save meta data about training the agent
        self.history_train = []

        self.state_info = dict()
        self.action_info = dict()

        self.last_reset_time = -np.infty

    @abstractmethod
    def train(self, ep_num):
        pass

    @abstractmethod
    def update_memory(self, action_operation, done, entities, reward, sensors, sim_time):

        # if is_new_action == True save data, else don't save the data

        # set is_new_action to false.
        pass

    @abstractmethod
    def forward(self,state):
        """
        runs the neural network and returns the output
        :param state:
        :return: raw action vector
        """
        pass

    def reset(self):
        # can be overridden by child classes
        self.reset_base()

    def reset_base(self):
        self.history_samples = []
        self.last_reset_time = -np.infty

    def normalize_state(self,entities, sensors):
        """
        Convert the numbers form the dimensional values in the simulation that are needed by the learning agent
        to a different scale. Typically the scales is [0,1] or [-1,1]

        :param entities:
        :param sensors:
        :return:
        """

        norm_state_dict = dict()
        state_dict = dict()
        for head_name, head_obs in self.observation_information.items():

            state = np.zeros(len(head_obs))
            norm_state = np.zeros(len(head_obs))
            for index, row in head_obs.iterrows():
                item = entities.get(row['name'],None)
                if item is None:
                    item = sensors.get(row['name'],None)
                norm_state[index] = ((item.state_dict[row['data']]-row['min']) / (row['max'] - row['min'])) * (row['norm_max']-row['norm_min']) + row['norm_min']
                state[index] = item.state_dict[row['data']]
            norm_state_dict[head_name] = norm_state
            state_dict[head_name] = state

        return norm_state_dict, state_dict

    def add_sample_history(self, reward, done):
        # init tuple
        mdp_tuple = dict()
        # add action information
        self.add_sample_history_helper(mdp_tuple, self.action_info)
        # add state information
        self.add_sample_history_helper(mdp_tuple, self.state_info)
        # add reward
        mdp_tuple['reward'] = reward
        # add terminal
        mdp_tuple['done'] = done

        self.history_samples.append(mdp_tuple)

    def add_sample_history_helper(self, mdp_tuple, info):
        for name, value in info.items():
            if hasattr(value, '__iter__'):

                if isinstance(value, dict):
                    for i, item in value.items():
                        for k, tmp_item in enumerate(item):
                            mdp_tuple[name +'_'+ str(i)+'_'+str(k) ] = tmp_item
                else:
                    # iterate through array and add to dict
                    for i, item in enumerate(value):
                        mdp_tuple[name + '_' + str(i)] = item
            else:
                mdp_tuple[name] = value

    def write_history(self,agent_name,episode_number, file_path):
        # TODO change from csv to sqlite data base
        # write history to csv
        df = pd.DataFrame(self.history_samples)
        file_path = os.path.join(file_path, 'learning_algorithm')
        df.to_csv(os.path.abspath(os.path.join(file_path,str(agent_name)+'_'+str(self.name)+'_epnum-'+str(episode_number)+'.csv')), index=False)

    def load_optimizer(self, name, opt_dict, params):
        #opt = self.h_params['Optimizer']
        if opt_dict['type'] == 'adam':
            self.optimizer[name] = optim.Adam(params,lr=opt_dict['lr'],betas=(opt_dict['beta1'],opt_dict['beta2']))
        else:
            raise ValueError('Optimizer not supported')

    def execute_action_operation(self, action_operation, controlled_entity, entities):

        # scale the output to the robots action?
        self.action_info['applied_action'] = action_operation.convert_action(self.action_info['mutated_action'])

        # apply the action
        entities[controlled_entity].apply_action(self.action_info['applied_action'])
