"""
Script that marches throught the training data saved from learning and extracts learning efficiency data.
"""

# native packages
import copy
from collections import Counter
import os

# 3rd party packages
from matplotlib.gridspec import GridSpec
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.integrate import trapezoid
from scipy.ndimage import uniform_filter1d
from scipy.signal import savgol_filter
import seaborn as sns

# own modules
from environment.NavigationEnvironment import NavigationEnvironment
from environment.Reward import ReachDestinationReward
from environment.Sensor import DestinationSensor
from environment.Termination import ReachDestinationTermination


def running_avg(data, window_size):

    avg = np.zeros_like(data,dtype=float)
    for i, tmp in enumerate(data):
        if i < int(window_size / 2):
            # tmp = cumulative_reward[0:window_size]
            sum_val = np.sum(data[0:window_size])
            avg_tmp = sum_val/window_size
            avg[i] = np.average(data[0:window_size])
        elif i > (len(data) - 1 - int(window_size / 2)):
            # tmp = cumulative_reward[len(cumulative_reward)-window_size:len(cumulative_reward)]
            avg[i] = np.average(data[len(data) - window_size:len(data)])
        else:
            # tmp = cumulative_reward[i-int(window_size/2):i+int(window_size/2)+1]
            avg[i] = np.average(data[i - int(window_size / 2):i + int(window_size / 2) + 1])

    return avg


def load_environment(base_folder, set_name, trial_num):
    file_name = os.path.join(base_folder, 'hyper_parameters.yaml')

    env = NavigationEnvironment()
    env.build_env_from_yaml(file_name,os.getcwd(), create=False) # TODO investigate correct input dir
    return env

def extract_train_data(base_folder, set_name, trial_num):
    """
    marches through the output files from a learning trial and saves the learning efficiency capabilities

    :param base_folder:
    :param set_name:
    :param trial_num:
    :return:
    """
    abs_path = os.getcwd().replace('\\analysis', '\\experiments')
    base_dir = os.path.join(abs_path, base_folder)
    base_dir = os.path.join(base_dir, 'output')
    base_dir = os.path.join(base_dir, set_name)
    base_dir = os.path.join(base_dir, str(trial_num))

    env = load_environment(base_dir, set_name, trial_num)

    # get the names of the agents
    file_dir = os.path.join(base_dir,'training', 'learning_algorithm')
    dir_list = os.listdir(file_dir)
    dir_list = [i for i in dir_list if 'loss' not in i]
    for i, file in enumerate(dir_list):
        dir_list[i] = file.split('_epnum')[0]

    agent_names = list(Counter(dir_list).keys())

    # get batch size and n_batches
    # TODO change to allow for multiple agents, not just the first one. forced 'alg0' agent currently
    if env.h_params['LearningAgent'].get('LearningAlgorithm', None) is None:
        n_batches = None
        batch_size = None
    else:
        n_batches = env.h_params['LearningAgent']['LearningAlgorithm']['alg0']['num_batches']
        batch_size = env.h_params['LearningAgent']['LearningAlgorithm']['alg0']['batch_size']

    # get goal distance
    goal_dst = None
    for name, reward_comp in env.reward_function.reward_components.items():
        if isinstance(reward_comp, ReachDestinationReward):
            goal_dst = reward_comp.goal_dst

    if goal_dst is None:
        # search in termination function for non-learning agent
        for name, term_func in env.termination_function.components.items():
            if isinstance(term_func, ReachDestinationTermination):
                goal_dst = term_func.goal_dst

    cols = ['ep_num', 'data_generated', 'data_trained_on','success','crash','reward','dst_traveled[m]']
    df_eff = pd.DataFrame(data=np.zeros((env.h_params['MetaData']['num_episodes'], len(cols))), columns=cols)
    n_data_generated = 0
    # loop over episode number
    for ep_num in range(env.h_params['MetaData']['num_episodes']):
        # save episode number
        df_eff['ep_num'].iloc[ep_num] = ep_num

        # open the learning agent file and get the amount of dtata
        # TODO need to iterate over agents and adjust script to save agents information in its own columns
        if batch_size is not None:
            file_name = os.path.join(file_dir,agent_names[0]+'_epnum-'+str(ep_num)+'.csv')
            tmp_df = pd.read_csv(file_name)

            # add the number of samples generated by the agent
            n_data_generated += len(tmp_df)
            df_eff['data_generated'].iloc[ep_num] = n_data_generated

            # get the amount of data trained on. Need to respect if the agent has learned enough data to start training
            if n_data_generated < batch_size:
                df_eff['data_trained_on'].iloc[ep_num] = 0.0
            else:
                if ep_num == 0:
                    df_eff['data_trained_on'].iloc[ep_num] = n_batches*batch_size
                else:
                    df_eff['data_trained_on'].iloc[ep_num] = df_eff['data_trained_on'].iloc[ep_num-1] + n_batches*batch_size

            # get reward accumulation
            df_eff['reward'].iloc[ep_num] = tmp_df['reward'].sum()

            is_done = tmp_df['done'].iloc[len(tmp_df)-1]

        # get distance travelled
        entity_name = ''
        for tmp_name, tmp_value in env.agents.items():
            if tmp_value.name in agent_names[0]:
                entity_name = tmp_value.controlled_entity
                break

        file_name = os.path.join(base_dir, 'training', 'entities', entity_name +'_epnum-' + str(ep_num) + '.csv')
        tmp_df = pd.read_csv(file_name)
        total_dst = 0.0
        # result = [total_dst + np.sqrt() for x, y in zip(tmp_df['col1'], tmp_df['col2'])]
        # result = [total_dst + np.sqrt() for i, row in enumerate(tmp_df[['x_pos','y_pos']].to_numpy())]
        for k in range(len(tmp_df)):  # TODO find a way to use list comprehensions or vectorization
            if k != 0:
                total_dst += np.sqrt((tmp_df['x_pos'].iloc[k] - tmp_df['x_pos'].iloc[k - 1]) ** 2 + (
                            tmp_df['y_pos'].iloc[k] - tmp_df['y_pos'].iloc[k - 1]) ** 2)
        df_eff['dst_traveled[m]'].iloc[ep_num] = total_dst

        # get success rate
        file_name = os.path.join(base_dir, 'training', 'sensors', 'destination_sensor_0_epnum-' + str(ep_num) + '.csv')
        tmp_df = pd.read_csv(file_name)
        min_dst = tmp_df['distance'].min()
        success = 0.0
        if min_dst <= goal_dst:
            success = 1.0
        df_eff['success'].iloc[ep_num] = success
        if batch_size is None:
            max_sim_time = tmp_df['sim_time'].iloc[len(tmp_df)-1]
            is_done = True
            if np.abs(max_sim_time-env.h_params['MetaData']['training_sim_time']) < 1e-5:
                is_done = False

        # get crash rate
        crash = 0.0
        if is_done and min_dst >= goal_dst:
            crash = 1.0
        df_eff['crash'].iloc[ep_num] = crash

    # calulate rolling averages
    window_sizes = [21,51,101,201]
    for ws in window_sizes:
        df_eff['success_avg_'+str(ws)] = running_avg(df_eff['success'], ws)
    for ws in window_sizes:
        df_eff['crash_avg_'+str(ws)] = running_avg(df_eff['crash'], ws)
    for ws in window_sizes:
        df_eff['dst_traveled[m]_avg_'+str(ws)] = running_avg(df_eff['dst_traveled[m]'], ws)
    for ws in window_sizes:
        df_eff['reward_avg_'+str(ws)] = running_avg(df_eff['reward'], ws)

    df_eff.to_csv(os.path.join(base_dir,'progress','Training_summary.csv'), index=False)

if __name__ == '__main__':

    base_folder = 'demo_to_test_boat_DDPG'
    set_name = 'DebugDDPGBSpline'
    trial_num = 9

    """
    Edit above ^^
    """

    extract_train_data(base_folder, set_name, trial_num)