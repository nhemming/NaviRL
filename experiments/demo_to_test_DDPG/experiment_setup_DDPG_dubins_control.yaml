Entities:
  MassFreeVectorEntity:
    name: learning_entity
    id: 0
    collision_object:
      #type: rectangle
      #height: 4.0  # [m]
      #width: 10.0  # [m]
      type: circle
      radius: 0.5 # [m]
  StaticEntity:
    name: destination
    id: 1
    #collision_object:
    #  type: circle
    #  radius: 1.0 # [m]
Environment:
  domain:
    min_x: 0 # [m]
    max_x: 5 # [m]
    min_y: 0 # [m]
    max_y: 5 # [m]
    buffer: 0.5 # [m]
  time_step: 0.5 # [s]
ExplorationStrategies:
  strategy1:
    name: EpsilonGreedy_0
    distribution: # mean, std of each action variable
      dist0: 0.0,0.1
      dist1: 0.0,0.0000001
      dist2: 0.0,0.1
      dist3: 0.0,0.0000001
    threshold_schedule:
      point0: 0,0.95 # x point, threshold for if a random number is added
      point1: 500,0.2
    type: EpsilonGreedy
LearningAgent:
  name: general_nav_0
  save_rate: 50
  type: SingleLearningAlgorithmAgent
  ActionOperation:
    alg_name: basic_control
    frequency: 2.0 # [s]
    is_continuous: True
    name: dubins_control

    # discreet solution
    number_controls: 1 # [-]
    segment_length: 1
    target_entity: learning_entity
    controller:
      type: pd
      p: 1.0
      d: 0.01

    # used to describe the options available for the discrete case and the bounds for each action
    # in the continuous actions.
    action_options:
      option0: -3.141592654,3.141592654 # describes the angle to the end point from the own principle axis.
      option1:  9.9999,10.0 # describes the distance the point is away from the own origin. Needs to be at least 4*r
      option2: -3.141592654,3.141592654 # describes the ending points angle reltive to the vector pointing to its base
      option3: 1.9999,2.0 #describes the radius [m] of the turns in the dubins path.

  ControlledEntity: learning_entity
  device: cuda
  LearningAlgorithm:
    alg0:
      batch_size: 512
      exploration_strategy: EpsilonGreedy_0
      gamma: 0.9
      name: DDPG_0
      num_batches: 32
      target_update_rate: 1
      tau: 0.025
      type: DDPG
      Input:
        head0:
          item0:
            name: learning_entity
            data: phi # heading
            min: -3.14159265359 # [rad]
            max: 3.14159265359 # [rad]
            norm_min: -1 # [-]
            norm_max: 1 # [-]
          item1:
            name: destination_sensor_0
            data: angle
            min: -3.14159265359 # [rad]
            max: 3.14159265359 # [rad]
            norm_min: -1 # [-]
            norm_max: 1 # [-]
          item2:
            name: destination_sensor_0
            data: distance
            min: 0 # [m]
            max: 20 # [m]
            norm_min: 0 # [-]
            norm_max: 1 # [-]
      NetworkActor:
        initializer: xavier
        head0:
          #batch_norm: False
          dropout: 0.0 # [-]
          hidden_activation: relu
          hidden_layers: 32 # 64,64
        tail:
          #batch_norm: False
          dropout: 0.0
          hidden_activation: relu
          hidden_layers: 32
          last_activation: tanh
          # only needed for continuous case
          #output_range: -1,1 # [-]
      NetworkCritic:
        initializer: xavier
        head0:
          #batch_norm: False
          dropout: 0.0 # [-]
          hidden_activation: relu
          hidden_layers: 32 # 64,64
        tail:
          #batch_norm: False
          dropout: 0.0
          hidden_activation: relu
          hidden_layers: 32
          last_activation: none
          # only needed for continuous case
          #output_range: -1,1 # [-]
      Optimizer:
        beta1: 0.9
        beta2: 0.999
        learning_agent: DDPG_0
        lr: 0.001
        name: adam_0
        type: adam
      ReplayBuffer:
        buffer_size: 131584
        type: vanilla
  
MetaData:
  num_episodes: 1000
  seed: 0
  set: DebugDDPGDubins
  training_sim_time: 20.0
  trial_num: 0

RewardDefinition:
  component0:
    adj_factor: 1.0
    destination_sensor: destination_sensor_0
    goal_dst: 1.0
    reward: 1.0
    target_agent: general_nav_0
    target_lrn_alg: DDPG_0
    type: reach_destination
  component1:
    adj_factor: 0.01
    destination_sensor: destination_sensor_0
    target_agent: general_nav_0
    target_lrn_alg: DDPG_0
    type: heading_improvement
  component2:
    adj_factor: 0.1
    destination_sensor: destination_sensor_0
    target_agent: general_nav_0
    target_lrn_alg: DDPG_0
    type: close_distance
  component3:
    adj_factor: 0.01
    aligned_angle: 0.087264
    aligned_reward: 1.0
    destination_sensor: destination_sensor_0
    target_agent: general_nav_0
    target_lrn_alg: DDPG_0
    type: aligned_heading
  overall_adj_factor: 1.0
Sensors:
  sensor0:
    type: destination_sensor
    name: destination_sensor_0
    id: 0
    owner: learning_entity
    target: destination
TerminationDefinition:
  component0:
    destination_sensor: destination_sensor_0
    goal_dst: 1.0
    name: reach_destination_0
    target_agent: general_nav_0
    type: reach_destination
  component1:
    name: any_collisions_0
    target_agent: general_nav_0
    type: any_collisions